#[1,2]#
1):[[1,0,0,0],
    [0,1,0,0],
    [0,0,1,0]]
2):지역 최솟값 부근에서 빠져나오지 못할 수도 있다.
3):X
4):CPU는 연산을 하나씩 처리하고 GPU는 여러개의 병렬연산을 한다.
5):[[19, 32],
    [18, 25]]
6):2, float
7):my_slice = train_images[ : , 14 : -7, 14 : -7 ]
8):배치, 빠르지만, 느리다, 배치 경사하강법
9):훈련 데이터, 손실 함수, 옵티마이저
10):transpose
11):지도학습, 비지도학습, 강화학습
12):모멘텀
13):여러개의 층으로 깊게 구성됨
14):CUDA는 GPU를 이용해 병렬연산처리를 하고, CUDA C는 CUDA를 구현하기 위한 C와 유사한 언어이다.
15):[[1,4],
     [2,5],
     [3,6]]
16):거짓
17):1, 4
18):역전파
#[3,4]#
1):손실의 현재 기울기보다 이전 기울기에 비중을 두어 그래프의 변동폭을 줄여 부드럽게 그려준다.
2):강화학습은 무작위적 행동을 한다.
3):무한대
4):len(sequence)
5):병목현상을 방지하기 위해
6):충분한 데이터를 더 모으기, 가중치 규제, 드롭아웃, 네트워크 크기 축소
7):binary_crossentropy
8):클래스 이용 객체 생성, 함수형 API
9):병목현상 방지
10):softmax
11):mae
12):강화학습은 무작위적 행동을 한다.
13):1, 5
14):5
15):24개
16):O, X, X
17):다른 관측치들과 동떨어진 관측치
18):데이터들의 값을 0을 평균으로 하며 표준편차를 +1, -1로 만든다
19):3, 5
20):빨간선
#[5,6]#
1):복잡한 데이터를 축소시키는 것, 풀링 연산
2):288
3):2
4):X
5):CNN, RNN
6):3
7):컨볼루션층의 노드(필터)수가 너무 많아 과적합이 될 가능성이 높다, 드롭아웃을 사용하거나 노드수를 줄인다.
8):한 번 증식시킬 때 증식시킬 이미지 수
9):4
10):이전 순서를 잘 처리하지 못한다.
11):forget gate - 이전 상태 데이터들을 얼마나 잊을지 결정
     input gate - 현재 입력 데이터를 얼마나 기억할지 결정
     output gate - 현재 입력을 바탕으로 출력을 결정
12):단어, 문장의 의미 정보
13):연산량을 줄여 연산 속도를 빠르게한다.
14):
15):one to many, many to one, many to many
16):0 0 0   0 0 0   0 4 5   5 6 0
     0 1 2   2 3 0   0 7 8   8 9 0
     0 4 5   5 6 0   0 0 0   0 0 0
17):X
18):X
#[7,8]#
1):리스트 데이터 입력, 딕셔너리 데이터 입력
2):5
3):5
4):낮은
5):5
6):X, X
7):5
8):검증손실이 3번 연속으로 향상되지 않으면 훈련 중지
9):O
10):하이퍼 파라미터는 사용자가 수정을 할 수 있고, 파라미터는 불가능
11):인코딩을 통해 잠재공간을 생성하고 그 잠재공간을 바탕으로 디코딩을 통해 결과를 출력
12):X
13):그람 행렬
14):그래디언트 소실, 역전파
15):1, 5
16):Parametric method
17):4
18):압축
19):생성자 : 판별자,
     판별자 : 판별 결과(손실)