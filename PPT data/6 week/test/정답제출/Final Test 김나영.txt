#[1,2]#
1):[1, 0, 0, 0]
   [0, 1, 0, 0]
   [0, 0, 1, 0]
2):전역최솟값이 아닌 지역 최솟값에서 학습이 멈출 수 있다.
3):X
4):CPU는 데이터를 직렬로 처리하고 GPU는 병렬로 한꺼번에 처리해서 더 빠르다.
5):[[19, 32],
    [18, 25]]
6):(1) : 2
   (2) : float
7):my_slice = train_images[-7:-14 , -7:-14 , : ]
8):배치,  , 오래걸린다, 배치, 서로의 단점을 보완한
9):데이터, 모델, 
10):transepose
11):
12):학습률을 더 높게 설정한다.
13):깊게 학습한다.
14):CUDA는 GPU연산을 처리하기 위해 NVIDIA에서 만든 것이고 이 연산을 처리하는 언어가 CUDA C이다
15):[[1,4],
    [2,5],
    [3,6]]
16):거짓
17):1, 4
18):학습

#[3,4]#
1):현재값에 비중을 약간 두고 이전값에도 비중을 두어 순간적으로 변화가 크게 나타나는 것을 방지한다.
2):강화학습은 비지도학습과 다르게 무작위적으로 행동을 더한다.
3):무한대
4):len(sequences)
5):데이터의 크기가 큰 경우 중간층이 작으면 학습을 할 때 데이터를 모두 충분히 학습시킬 수 없다. 데이터 병목현상 발생
6):드롭아웃 추가, 가중치 규제 적용, 데이터를 더 모은다, 네트워크 용량 감소
7):binary crossentropy
8):
9):데이터의 크기가 큰 경우 중간층이 작으면 학습을 할 때 데이터를 모두 충분히 학습시킬 수 없다. 데이터 병목현상 발생
10):sigmoid
11):
12):강화학습은 비지도학습과 달리 무작위적으로 행동을 추가한다.
13):5
14):5
15):24
16):O, X, X
17):다른 관측치들보다 많이 벗어나 있는 데이터
18):정규화
19):2,4
20):빨간선

#[5,6]#
1):데이터의 크기가 큰 값을 축소시키는 것이다. MaxPooling등이 있다.
2):288
3):2
4):X
5):
6):3
7):
8):훈련시킬 데이터를 한번에 넣지 않고 배치 사이즈만큼 나누어서 넣는다는 뜻이다
9):2,5
10):(1) : 1D-ConvNet은 처리 속도가 빠르다
11):input gate, output gate, 
12):서로 연관성이 높은 단어들끼리 모을 수 있다.
13):패딩을 추가한 이미지에 3X3 필터를 적용하면 입력과 출력의 크기가 같기 때문
14):
15):
16):0 0 0  |  0 0 0     
     0 1 2  |  2 3 0
     0 4 5  |  5 6 0
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
     0 4 5  |  5 6 0
     0 7 8  |  8 9 0
     0 0 0  |  0 0 0
17):X
18):O
#[7,8]#
1):
2):5
3):5
4):낮은
5):5
6):O, X
7):5
8):3번까지 학습하고 멈춘다
9):O
10):파라미터는 데이터에 의해 알아서 조정되는 것이고 하이퍼파라미터는 사용자가 직접 조절해준다
11):인코더와 디코더를 통해 원본이미지의 크기를 압축한다.
12):X
13):
14):그래디언트 손실, 역전파
15):1,5
16):Parametric method
17):4
18):데이터를 압축시키는
19):생성자는 판별자로부터 피드백 받고 판별자는 정답값으로부터 피드백받는다.
