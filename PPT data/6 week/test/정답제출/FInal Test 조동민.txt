#[1,2]#
1):
2): 전역 최솟값이 아닌 지역 최솟값에 도달할 가능성이 크다.
3): O
4): CPU보다 GPU의 연산 속도가 빠르다.
5):
6):
7): [ : , 14 : -7 , 14 : -7 ]
8): 배치, 빠르지만, 느리다, 배치
9): 입력데이터, 기대출력,  성능측정방법
10):  x = np.transpose(x)
11): 지도학습 비지도학습 강화학습
12): 모멘텀
13): 신경망의 층이 깊다는 뜻.  즉 많은 층을 가지고 있다란 뜻
14):  CUDA 는 GPU에서 수행하는 알고리즘을 사용하여 작성할 수 있도록 하는 기술
       CUDA C는 GPU를 사용하여 응용 프로그램을 작성하기 위한  언어
15): [[1,4]
       [2,5]
       [3,6]]
16): 거짓
17): 1, 5
18): 역전파알고리즘

#[3,4]#
1): 이전 값들의 평균에 비중을 두고 새로운 값에는 약간의 비중을 두어 
    순간 변폭을 줄여 그래프에 전체적인 변화를 중시하도록 표기하는 것
2): 강화학습은 랜덤적인 요소를 계속해서 시도한다.
3): 무한대의
4): len(sequences)
5): 많은 수의 범주를 분류할 때 중간층의 크기가 작으면 네트워크에 정보의 병목이 생기기 때문
6): 훈련 데이터를 더 모은다.
    네트워크의 용량을 감소시킨다.
    가중치 규제를 추가한다.
    드롭아웃을 추가한다.
7): 바이너리 크로스엔트로피
8): 모델을
9): 많은 수의 범주를 분류할 때 중간층의 크기가 작으면 네트워크에 정보의 병목이 생기기 때문
10): 소프트맥스
11): 시그모이드
12): 강화학습은 랜덤적인 요소를 계속해서 시도한다.
13): ⑤ 학습을 더 수행하기
14): ⑤ 가위바위보 이미지 분류에서 20%의 정확도를 달성하였다.
15):
16): O, X, O
17):
18):
19): ② 가중치 규제는 가중치 값의 분포가 균일해지도록 하는 기법이다.
③ 가중치 규제 중 L1 규제는 가중치의 제곱합를 비용함수에 추가하고, 
L2 규제는 가중치의 절댓값 합계을 비용함수에 추가한다.
20): 검정색 선
 
#[5,6]#
1): 어떤 자료에서 일부 값을 추출하는 것을 의미
2): 1024
3): 3)3E-3 은 0.003을 말한다
4): O
5):
6): 3) 1D컨브넷과 2D컨브넷은 전혀 다르게 구성된다.
7): 컨볼루션 레이어의 사이즈를 줄여준다.
8): 배치의 크기, 각 배치당 학습할 크기
9): 3) 히트맵 시각화는 이미지처리 모델의 회귀 및 분류에 대해 판단 근거를 제시한다고 할 수 있다.
10):
11):  
12): 
13): 더 적은 수의 파라미터를 가지기 때문
14): 정확도, 속도, 크기
15): one-to-many / many-to-one / many-to-many
16):
17): X
18): X

#[7,8]#
1):
2): 5
3): 5
4): 높은
5): 5
6): X, O
7): 4
8):
9): X
10): 하이퍼 파라미터는 모델링할 때 사용자가 직접 세팅해주는 값.
      즉, 훈련되어질 파라미터가 아닌 구조에 관련된 파라미터
11): 입력 이미지와 동일한 이미지를 타깃 데이터로 사용하여 훈련하는 생성모델
      즉, 원본 입력을 재구성하는 방법을 학습하는 모델
12): O
13): 그람행렬
14):
15): 1, 5
16): Nonparametric method
17): 4
18): 빠른
19):

			
			
			
