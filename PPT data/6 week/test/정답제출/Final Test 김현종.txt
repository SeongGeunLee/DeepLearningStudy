#[1,2]#
1): [0001][0010][0100]
2): 지역 최소값에서 머무를 수 있다
3): X
4): GPU는 CUDA코어를 사용해서 더 빠른 연산이 가능하다
5): [[19, 32], [18, 25]]
6): (1) 2 (2) float
7): my_slice = train_images[:, -14 : -7, -14 : -7]
8): 배치, 모델이, 연산이 오래걸림, 배치
9): 지도학습, 비지도학습, 강화학습
10): transpose
11): 지도학습, 비지도학습, 강화학습
12): 모멘텀
13): 여러 계층을 통해서 학습을 하는 방법
14): CUDA는 빠른 연산을 위한 GPU에 있는 코어 이고 CUDA C는 이를 사용하기 위한 언어이다
15): [[1,4],[2,5],[3,6]]
16): 거짓
17): 4,5
18): 
#[3,4]#
1): 전체 데이터가 변동이 너무 심해서 오래된 데이터는 적게 반영하고 최근 데이터는 많이 반영 한것
2): 강화학습은 랜덤한 동작이 나타남
3): 0
4): len(sequences)
5): 중간층이 작을 경우 일부 특성이 소실될 수 있기 때문이다
6): 네트워크 크기 축소, 규제 추가, 드롭아웃 추가, 데이터 양 늘림
7): binary_crossentropy
8): sequential, 함수형 api
9): 중간층이 작을 경우 일부 특성이 소실될 수 있기 때문이다
10): sigmoid
11): 평균절대 오차/평균제곱 오차
12): 강화학습은 랜덤한 동작이 나타남
13): 5
14): 5
15): 24
16): o,x,x
17): 관측치 값과 원래 결과 값과의 차이
18): 값을 정규화 한다.
19): 1, 3, 4
20): 빨간 선
#[5,6]#
1): 특징을 강화하고 데이터 사이즈를 줄이는 것 maxpooling
2): 128
3): 2
4): x
5): 1)CNN 2)RNN
6): 3
7): 층마다의 은닉유닛 개수가 같다. 은닉유닛 개수를 수정한다
8): 한번 데이터를 처리할 때 처리할 데이터의 양
9): 4
10): (1)CNN이 데이터 처리 속도가 빠르기 때문에 (2)시계열 데이터 처리를 하기에는 RNN같은 순서감지가 안됨
11): 데이터가 input gate를 통해 입력이 되고 forget gate를 통해 기억할 데이터를 거른 후 이전 상태와 합쳐지고 output gate를 통해 나간다
12): 비슷한 단어들 끼리의 의미, 방향성?
13): 연산양이 많지 않고 특성을 도출해내기 좋아서
14): 현재 사용하려는 모델에 적합한 의미를 갖고 있는지
15): simple rnn, lstm, gru
16): [[000][012][045]], [[000][230][560]], [[045][078][000]], [[560][890][000]]
17): x
18): x
#[7,8]#
1): 함수형 api
2): 5
3): 4
4): 낮은
5): 4
6): o,x
7): 3
8): 3에포크동안 모델이 발전하지 않으면 훈련을 중지한다
9): o
10): 하이퍼 파라미터란 모델의 더 나은 학습을 위해 조절하는 인자이다.
11): 
12): o
13): 
14): 그래디언트 소실 문제, 잔차연결
15): 1,5
16): Nonparametric method
17): 1
18): 특성추출
19): 생성자 - 판별자 판별자 - 이미지 정답값
