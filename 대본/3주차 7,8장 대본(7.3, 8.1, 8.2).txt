여기서 2가지 디자인 패턴  배치 정규화, 깊이별 분리 합성곱은
고성능 심층 컨브넷을 만들 때 유용합니다. 컨브넷은 합성곱 신경망
이라고도 하는데 여기서 합성곱 신경망은 시각적 이미지들
사진, 그림 이런 이미지들을 분석하는데 사용되는 신경망입니다.
심층은 말 그대로 층이 깊다. 이런 말입니다. 그래서 층이 깊은
합성곱 신경망을 만들때 유용한 패턴들이 바로 저 위에있는
2가지 배치정규화와, 깊이별 분리 합성곱입니다.

먼저 정규화는 모델에 주입되는 샘플들을 균일하게 만드는 방법입니다.
코드를 보면 데이터에서 평균을 빼고 np.mean이 data의 평균을 구해
줍니다. 그리고 np.std는 data의 표준편차를 구해줍니다. 그래서 
평균을 빼고 표준편차로 나누어서 데이터의 분산을 1로 만들어 줍니다.

그래서 배치 정규화는 층을 기준으로 입력 데이터를 정규화
하는것 입니다. 위에 코드는 합성곱층 다음 배치 정규화를 사용한
코드이고 아래는 완전연결층 다음에 배치 정규화를 사용한 코드입니다.

다음은 깊이별 분리 합성곱입니다. 얘는 일반적인 합성곱이 각 채널만의
공간적 특징을 추출 하는 것이 불가능 하기 때문에 고안해낸 방법입니다.
그렇다면 깊이별 분리 합성곱에서 하려고 하는것은 각 채널마다의
공간적 특징을 추출하는 것입니다. 따라서 여기서는 각 채널마다 필터가 존재
하게되고, 이러한 특징 때문에 input과 output의 채널이 같게 됩니다.

이 코드에서 깊이별 분리 합성곱층을 사용하려면 conv2D대신에
세퍼러블Conv2d를 사용하면 됩니다.

다음은 하이퍼 파라미터 최적화 입니다. 하이퍼 파라미터는 간단하게
말하면 모델링할 때 사용자가 직접 세팅해주는 값을 뜻합니다.
예를들면 학습률, 층의 개수, 노드의 수 ,필터수 등등이 다 하이퍼 파라미터입니다.
그럼 파라미터와 하이퍼 파라미터의 차이는 뭘까?
파라미터와 하이퍼파라미터를 구분하는 기준은 사용자가 직접 설정하느냐 아니냐
입니다. 사용자가 직접 설정하면 하이퍼 파라미터이고, 모델 혹은
데이터에 의해 결정되면 파라미터입니다.

전형적인 하이퍼 파라미터 최적화 과정은 일련의 하이퍼파라미터를
자동으로 선택합니다. 그리고 선택된 하이퍼 파라미터로 모델을 만들고
훈련 데이터에 학습하고 검증데이터에서 최종 성능을 측정합니다.
다음으로 시도할 하이퍼 파라미터를 자동으로 선택합니다. 이 과정을 계속 반복하고
마지막으로 테스트 데이터에서 성능을 측정합니다.
주어진 하이퍼 파라미터에서 얻은 검증 성능을 사용하여 다음 번에 시도할 
하이퍼 파라미터를 선택하는 알고리즘이 이 과정의 핵심입니다.

앙상블은 여러 개 다른 모델의 예측을 합쳐서 더 좋은 예측을 만듭니다.
앙상블은 조화 또는 통일을 의미하는데 여러가지 동일한 종류의
혹은 서로 상이한 모형들의 예측/분류 결과를 종합하여 최종적인
의사결정에 활용하는 방법론입니다.
다양한 모형의 예측 결과를 결합함으로써 단일 모형으로 분석했을 때 보다
신뢰성 높은 예측값을 얻을 수 있습니다. 책에 나오는 장님과 코끼리 이야기는
한 장님은 코끼리 코만 만지고 뱀인줄 알고, 다른 장님은 코끼리 다리만 만져서
나무인줄 알았지만 코끼리를 만진 여러 장님들 끼리 모여서 이야기를 하면
코끼리를 매우 정확하게 묘사 할수 있는것 처럼 다양한 앙상블의 후보모델의
예측값을 합치면 더욱더 정확도가 올라갑니다.
여기서 모델 앙상블의 핵심은 최상위 모델이 얼마나 좋은지 보다 앙상블의
후보 모델이 얼마나 다양한지가 더 중요합니다.

#LSTM으로 텍스트 생성하기
여기서는 순환 신경망으로 시퀀스 데이터를 생성하는 방법에 대해서
다룹니다. 음악을 만들거나, 그림을 그리거나, 텍스트를 생성하는 것 등등 입니다.

시퀀스 데이터를 생성하는 일반적인 방법은 이전 토큰을 입력으로
사용해서 시퀀스의 다음1개 또는 몇 개의 토큰을 예측하는 것입니다.
그림에서 보면 the cat sat on the m으로 초기 텍스트가 주어지고 언어 모델을
거치게 되는데 여기서 언어 모델은 이전 토큰들이 주어졌을 때 다음 토큰의
확률을 모델링 할 수 있는 네트워크입니다. 그래서 다음 토큰의 확률을
모델링 해주는 네트워크를 이용해서 초기 텍스트에 대한 다음 글자의 확률
분포를 구하게 됩니다. 그리고 여기서 확률 분포를 토대로 샘플링 전략에
따라서 다음 글자를 선택하게 됩니다. 여기서는 a가 선택이 되었습니다.
그리고 선택된 글자 a가 초기텍스트 뒤에 붙어서 the cat sat on the ma가
초기 텍스트가 되어서 다시 언어모델을 거치고 확률을 구하고
다음 글자를 선택하게 됩니다. 이런 과정을 반복하여 시퀀스데이터를
생성하게 됩니다.

샘플링은 어떤 자료에서 일부 값을 추출하는것을 의미합니다.
샘플링 방법은 2가지가 있는데 탐욕적 샘플링 방법과 확률적 샘플링 방법이
있습니다. 탐욕적 샘플링은 가장 높은 확률을 가진 글자를 선택하는 방법입니다.
만약에 저 확률 분포가 abcde가 될 확률이라고 했을때 탐욕적 샘플링을
사용해서 글자를 선택한다면 가장 확률이 높은 b가 다음글자로 선택이 될겁니다.

확률적 샘플링은 확률에 따라 글자를 선택하는 방법입니다.
a가 50%이고 b가 10%라고 한다면 50%의 확률로 a가 선택되어지고 10%의
확률로 b가 선택이 되어 집니다.

그리고 소프트맥스 온도는 샘플링 과정에서 확률의 양을 조절하기 위해
사용합니다. 샘플링에 사용되는 확률 분포의 엔드로피를 나타냅니다.
여기서 엔드로피는 확률 분포가 가지는 정보의 확신도, 혹은 정보량을 수치로
표현 한 겁니다. 확률분포에서 특정한 값이 나올 확률이 높아지고 나머지
값의 확률은 낮아진다면 엔드로피가 작아지고, 반대로 여러가지 값이 나올 확률이
대부분 비슷한 경우에는 엔드로피가 높아집니다. 엔드로피는 확률분포의 모양이
어떤지를 나타내는 특성값 중 하나로 볼 수도 있습니다. 확률 또는 확률밀도가
특정값에 몰려있으면 엔트로피가 작다고 하고 반대로 여러가지 값에 골고루 퍼져
있다면 엔트로피가 크다고 합니다.
확률분포의 엔트로피는 물리학의 엔트로피 용어를 빌려온건데 물리학에서는
물질의 상태가 여러가지로 고루 분산되어있으면 엔트로피가 높고 특정한
하나의 상태로 몰려있으면 엔트로피가 낮습니다. 이와 비슷하게 
더 많은 단어가 선택될 가능성이 있다면 엔트로피가 높아지고
만약에 한단어만 선택될 가능성이 있다면 엔트로피가 낮아집니다.

여기서 높은 온도는 엔트로피가 높은 샘플링 분포를 만드는데
보통 높은 온도에서는 즉, 엔트로피가 높은 상태에서는 다양한 단어가
선택될 기회가 있으니까 더 창의적이고
생소한 데이터를 생성해내고 낮은온도 즉, 엔트로피가 낮은 상태에서는
무작위성이 없기 때문에 창의적인 내용을 만들어내지 못합니다.

이번에는 글자수준의 LSTM텍스트 생성 모델을 구현하는 과정입니다.
여기서는 독일의 철학자 니체의 글을 사용했습니다. 그래서 먼저 필요한 모듈을
임포트해주고 origin에 해당하는 주소에 니체의 글이 있습니다.
그래서 주소로 부터 글을 가져와서 니체.txt파일을 생성하고
그 파일에 가져온 글을 저장하는 코드입니다.
그리고 path에 저장된 txt파일을 열고 읽어서 lower()로 소문자로
바꾼다음에 text에 저장을 했습니다.

다음은 text를 출력해 봤는데 긴 문장이 들어가 있습니다.

여기서 maxlen을 60으로 주고 step을 3으로 주었습니다.
처음 for문은 0~ 텍스트의 길이-60만큼 반복을 하게 됩니다. 그런데
step이 3이기 떄문에 i는 0 3 6 9 이런식으로 3씩 증가하게 됩니다.
이 코드를 예로 들어보면 apple이 text에 저장 되어있다고 하고
maxlen=3, step=1 이라면 sentences에는 리스트로 app, ppl, ple 이렇게 저장이
됩니다. 그리고
chars 에 대한 코드를 보면 set함수를 사용했는데 set함수는 고유한 문자를
추출해내는 함수입니다. 그래서 고유한 문자를 추출해 내고 list로 변환시켜주고
정렬한 코드입니다. 그리고 다음 char_indices는 고유한 글자를 딕셔너리로
저장시켜준 코드입니다. 
예를 들면 apple의 고유한 글자는 a, p , l , e 총4개 입니다. 그래서
딕셔너리로 저장을 하게 되면 a:0, p:1, l:2 ,e:3 으로 저장이 됩니다.
그리고 마지막에 벡터화를 해줍니다.
추출한 시퀀스를 보면 sentence의 인덱스0과 1을 출력을 해봤는데
sentences의 인덱스0번째는 text에 저장되어있는 처음 글자부터 60개가 저장이
되어있고 인덱스 1번째에는 3칸앞으로 이동해서 maxlen이 60이므로 
60개의 글자가 저장이 되어있습니다. 
옆에는 길이를 출력해봤는데 60의 길이로 나옵니다.
그리고 아래는 고유한 글자를 set함수를 이용해서 추출해낸 모습입니다.
마지막에 이상한 글자가 섞여있는데 이거는 txt파일을 read하는 과정에서
글자가 깨진것 같습니다. 원래는 안깨져야 정상입니다.

그리고 이거는 고유한 글자를 딕셔너리로 변경한 char_indices
를 출력해봤습니다. 이런식으로 나오게 됩니다.

이제 데이터 전처리 단계가 끝나고 네트워크 구성단계입니다.
첫번째 코드로 모델을 생성하고 두번째 코드에서 LSTM을 사용했습니다.
그리고 마지막에 Dence층은 고유한 글자수만큼 노드의 개수를 정했고
마지막 출력을 활성화함수 소프트맥스로 묶어 주었습니다.

그아래코드는 옵티마이저를 RMSprop를 사용하였고 학습률을 0.01로 주었습니다.
그리고 손실함수로 카테고리컬 크로스 엔트로피를 사용했습니다.

그리고 모델의 예측이 주어졌을 때 새로운 글자를 선택하는 함수를
만들어주었습니다.

이 코드는 랜덤시드를 42로 설정해주었고
0부터 text의길이-maxlen-1사이의 수를 랜덤으로 선택해서 시작인덱스로
정합니다. 
다음 for문은 60번 반복하는 for문입니다.
그리고 아래for문은 온도를 0.2 0.5 1.0 1.2로 선택하여 각각 돌려봤습니다.
다음에 400번 반복시켜서 400개의 글자를 생성하는 코드입니다.

60에포크, 온도가 0.2일때와 0.5일때 결과를 나타냅니다.
텍스트 생성에 가장 좋은 온도는 0.5일때 결과가 제일 좋습니다.
시드 텍스트는 the slowly부터 for까지 랜덤으로 설정되었습니다.
시드를 42말고 다르게 지정하면 시드텍스트가 변경됩니다.

#딥드림
딥드림은 합성곱 신경망이 학습한 표현을 사용하여 예술적으로 이미지를
조작하는 기법입니다. 옆에 그림은 딥드림을 사용해서 출력한 이미지입니다.
딥드림은 최대값을 찾는 경사 상승법을 적용합니다.

이번 예제에서는 케라스의 인셉션v3 모델을
사용하여 딥드림을 구현했습니다.
여기서 인셉션v3는 먼저 딥러닝은 많은 데이터를 기반으로 학습을 시켜
각 함수의 가중치값인 weight값을 정하는 과정이라고 볼 수 있었습니다.
또한 CNN기본구조에서 다루었던 각 층들, 합성곱,풀링,전겹합층 등의 구조나
구성에 따라 CNN의 여러 종류가 존재할 수 있습니다. 이번에 다루고자
하는 인셉션V3는 구글에서 미리 이러한 weight값들과 label값들을 만들어 놓은
버전입니다. 인셉션 v3모델은 총 1000개의 label값을 갖고 있으며 이미지넷
챌린지의 이미지 매칭 정확도 평가에서 96%이상을 보여줍니다.

처음에 인셉션v3와 백엔드를 임포트 시켜주고 아래에 K.set코드는
모든 새로운 연산은 테스트모드 즉, 모델을 훈련하지 않겠다 의미입니다.
만약에 1을 넣으면 훈련모드가됩니다.
그리고 아래에 합성곱 기반 층만 사용한 인셉션v3네트워크를 만들어주고
사전훈련된 이미지넷 가중치를 사용합니다.
include_top은 전결합층 계층들을 포함 시킬지의 여부인데
false를 넣었기 때문에 전결합층을 사용하지 않는다는 의미입니다.

아래 코드는 인셉션v3 모델에 있는 층의 이름에 활성화가 기여할 양을
정해줬습니다.

첫줄은 층 이름과 층 객체를 매핑한 딕셔너리를 만들어줍니다.
그리고 loss를 정의하고 
for문을 통해 마지막에 loss에 층 특성의 가중치의 
파라미터를 모두 제곱하여 더한 후 이 값의
제곱근을 구하는 L2노름 제곱을 추가합니다.
여기서 이미지 테두리는 제외했습니다.

경사상승법을 하는 코드입니다.
(1줄)이 텐서는 생성된 딥드림 이미지를 저장합니다.
(2줄)손실에 대한 딥드림 이미지의 그래디언트를 계산합니다.
(3줄)그래디언트를 정규화합니다.
(4줄)주어진 입력 이미지에서 손실을 만들고
(5줄)그래디언트 값을 계산할 케라스 Function 객체를 만듭니다.
아래 그래디언트_ascent함수는 경사 상승법을 여러번 반복하여 수행합니다.

이 코드에서 step에 경사상승법 단계크기를 지정해 주고
num_octave에 경사 상승법을 실행할 스케일 단계횟수를 지정합니다.
octave_scale은 스케일 간의 크기 비율이고
iterations은 스케일 단계마다 수행할 경사 상승법 횟수입니다.

max_loss에 손실이 10보다 커지면 경사 상승법 과정을
중지하기 위해서 10으로 지정했습니다.

base_image_path에 사용할 이미지가 저장된 경로를 넣고

img에 이미지를 넘파이배열로 로드합니다.

아래는 경사상승법을 실행할 스케일 크기를 정의한 튜플의 리스트를 준비합니다.
제일 작은 이미지에서 40%씩 커지는 크기의 리스트를 만들기 위해
역으로 가장 큰 원본 이미지 크기에서 1.4로 연속적으로 나눕니다.

successive_shapes에 이 리스트를 크기순으로 뒤집고
original_img에 img를 복사합니다. 그리고
shrunk_original_img에 img에 저장된 이미지를 가장 작은 스케일로 변경합니다.

이 코드는 350x350크기의 원본 이미지를 178x178크기로 줄여서 시작합니다.
그다음 40%씩 두번 늘려서 178x178에서 250x250으로 커지고
250x250에서 350x350 크기로 커집니다. 그래서 총 3번 딥드림을 수행합니다.

이 코드는 가장 작은 스케일 이미지와 원본 이미지를 현재
스케일로 바꾸어서 이 차이를 계산해줍니다.

이 함수는 유틸리티 함수들인데 resize_img는 이미지의
크기를 변경시켜주는 함수이고
save_img는 이미지를 저장해주는 함수이고
preprocess_image는 사진을 열고 크기를 줄이고 인셉션v3가 인식하는
텐서 포맷으로 변환하는 함수입니다.
deprocess_image는 넘파이 배열을 적절한 이미지 포맷으로 변환하는
함수입니다.
